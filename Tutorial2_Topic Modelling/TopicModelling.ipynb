{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PqUB3Vm0rZDo"
      },
      "source": [
        "<center><b>CSIE604284 â€¢ Analitika Media Sosial</b></center>\n",
        "<center><b>Fakultas Ilmu Komputer, Universitas Indonesia</b></center>\n",
        "<center><b>Self Paced Learning - Topic Modeling</b></center>\n",
        "\n",
        "<b>Instruksi Pengerjaan</b>:\n",
        "* Self learning ini juga termasuk tugas\n",
        "* Tugas ini adalah tugas individu namun Anda diperbolehkan berdiskusi dengan teman dalam proses pengerjaan tugas. \n",
        "* Anda diberikan berkas TopicModelling.ipynb dan kumpulan data\n",
        "* Tuliskan jawaban Anda pada berkas ini dan kumpulkan melalui SCeLE dengan format penamaan <b>TopicModelling_Nama_NPM.ipynb</b> sebelum <b>Senin, 21 Maret 2022 pukul 22:00.</b>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rDTRhZ3grb-I"
      },
      "source": [
        "# <b>A. Topic Modeling</b><br>\n",
        "    \n",
        "<i>Topic modeling</i> adalah suatu teknik <i>unsupervised learning</i> untuk mengekstrak topik-topik yang ada pada suatu kumpulan dokumen teks. Satu dokumen dapat memiliki lebih dari satu topik. <i>Topic modeling</i> dapat membantu mengeksplorasi dokumen teks dengan jumlah yang besar untuk menemukan kelompok kata, kesamaan antara dokumen, menemukan topik abstrak, melakukan <i>clustering</i> dokumen, dan sebagainya. Salah satu penerapannya adalah untuk menyusun buku-buku yang memiliki topik yang mirip berdasarkan isi buku-buku tersebut.  \n",
        "\n",
        "<img src=\"https://i.ibb.co/0srwgNv/blei.png\" alt=\"blei image\" style=\"width:75%; height:50%;\">\n",
        "<center>Sumber gambar: Blei, D. M. (2012). Probabilistic topic models. <i>Communications of the ACM, 55</i>(4), 77-84</center>\n",
        "\n",
        "Terdapat beberapa pendekatan dalam menentukan topik pada suatu teks, misalnya pendekatan Term Frequency and Inverse Document Frequency (TF-IDF), Non-Negative Matrix Factorization (NMF), Latent Semantic Analysis (LSA), dan Latent Dirichlet Allocation (LDA). Pada tutorial kali ini akan dilakukan <i>topic modeling</i> menggunakan pendekatan LDA. Pendekatan LDA dalam <i>topic modeling</i> menganggap setiap dokumen sebagai kumpulan topik dalam proporsi tertentu dan setiap topik sebagai kumpulan kata kunci dalam proporsi tertentu. Topik pada LDA merupakan kumpulan kata kunci dominan yang dapat mewakilkan suatu ciri khas. \n",
        "\n",
        "<img src=\"https://i.ibb.co/QKpbhQx/lda.png\" alt=\"lda image\" style=\"width:75%; height:50%;\">\n",
        "<center>Sumber gambar: http://chdoig.github.io/pytexas2015-topic-modeling/#/3/4</center>\n",
        "\n",
        "\n",
        "Pada self learning ini, Anda diminta untuk melakukan <i>topic modelling</i> menggunakan pendekatan LDA dengan dataset yang sudah disediakan. Langkah-langkah untuk melakukan <i>topic modelling</i> dengan pendekatan LDA menggunakan <i>dataset</i> tersebut adalah sebagai berikut:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9YFT-C9LreQN"
      },
      "source": [
        "## <b>1. Import library</b><br>\n",
        "Import seluruh <i>library</i> yang dibutuhkan pada tutorial ini.\n",
        "\n",
        "<b>#Code 1</b><br>\n",
        "<i>Code</i> ini digunakan untuk melakukan unduhan dan import library yang diperlukan"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YEj1dN8OrgGK"
      },
      "outputs": [],
      "source": [
        "!pip install nltk\n",
        "!pip install plotly\n",
        "!pip install numpy\n",
        "!pip install pandas\n",
        "!pip install pyLDAvis\n",
        "!pip install gensim\n",
        "!pip install scikit-learn\n",
        "!pip install tqdm\n",
        "\n",
        "import os\n",
        "import numpy as np\n",
        "import nltk\n",
        "import re\n",
        "import pandas as pd\n",
        "import plotly as py\n",
        "import matplotlib.pyplot as plt\n",
        "import plotly.graph_objs as go\n",
        "\n",
        "import pyLDAvis.gensim_models\n",
        "# jika masih error \"Module not found\" setelah instalasi pyldavis, \n",
        "# ubah import menjadi \"pyLDAvis.gensim\" dan semua kemunculan pyLDAvis di bawahnya\n",
        "\n",
        "from plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot\n",
        "from collections import OrderedDict, Counter\n",
        "from sklearn.decomposition import NMF\n",
        "from wordcloud import WordCloud\n",
        "from tqdm import tqdm\n",
        "\n",
        "from gensim import corpora, models, similarities\n",
        "from gensim.models import CoherenceModel\n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.cluster import KMeans\n",
        "\n",
        "nltk.download('punkt')\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VbgVpyTnrikR"
      },
      "source": [
        "## <b>2. Membaca data</b><br>\n",
        "Dataset yang digunakan adalah tweet yang diambil langsung dengan metode scraping melalui media sosial twitter dengan 3 keyword besar yang berbeda dengan rentang waktu 8 Februari 2022 hingga 8 Maret 2022. Setiap keyword dilakukan scrape secara terpisah dan kemudian masing - masing hasilnya digabungkan menjadi satu kesatuan dataset.\n",
        "\n",
        "Keyword yang digunakan pada dataset yang digunakan adalah:\n",
        "1. \"perang rusia ukraina\" -> Diterjemahkan sebagai \"perang AND rusia AND ukraina\"\n",
        "2. \"masker covid\" -> Diterjemahkan sebagai \"masker AND covid\"\n",
        "3. \"makan keluarga\" -> Diterjemahkan sebagai \"makan AND keluarga\"\n",
        "\n",
        "Hasil final dari data yang digunakan saat ini berupa data bernama dataset.csv yang berisi 41721 baris data. Dataset akan dibaca dengan menggunakan <i>library</i> bernama pandas.\n",
        "\n",
        "<b>#Code 2a</b><br>\n",
        "<i>Code</i> ini digunakan untuk mengambil data dari sumber utama serta membaca dataset utama yang berisi tweet yang akan diolah"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sLBPC42brlHg"
      },
      "outputs": [],
      "source": [
        "# Mengambil data\n",
        "!wget -q --no-check-certificate 'https://docs.google.com/uc?export=download&id=1rFLB_1QpZhKyWFGxqPo5znTEkh7dUhPf' -O dataset.csv\n",
        "!wget -q --no-check-certificate 'https://docs.google.com/uc?export=download&id=1xEIQwYre1SR71uRdQuez9MDmAUIysvAG' -O stopwordsID.csv\n",
        "!wget -q --no-check-certificate 'https://docs.google.com/uc?export=download&id=19NOzXA8Voturopg_DTuiMns3s4M2IAUz' -O kamus_singkatan.csv\n",
        "!wget -q --no-check-certificate 'https://docs.google.com/uc?export=download&id=1VjgivEr1pxyRCuyhVifPnaReFz0yd8Us' -O colloquial-indonesian-lexicon.csv\n",
        "\n",
        "# Membaca dataset\n",
        "datafile = 'dataset.csv'\n",
        "tweets = pd.read_csv(datafile)\n",
        "tweets = tweets.drop_duplicates(subset='text')\n",
        "tweets = tweets.assign(created_at=pd.to_datetime(tweets.created_at))\n",
        "print('Terdapat ' + str(tweets.shape[0]) + ' Baris data tanpa duplikat tweet yang akan diolah')\n",
        "tweets.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qamVA2B4rn-V"
      },
      "source": [
        "<b>#Code 2b</b><br>\n",
        "<i>Code</i> ini digunakan untuk membuat grafik terkait aktivitas tweet sepanjang waktu"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K-xYXKemrocT"
      },
      "outputs": [],
      "source": [
        "# Melihat aktivitas tweet sepanjang waktu\n",
        "tweets['created_at'] = pd.to_datetime(tweets['created_at'], format='%y-%m-%d %H:%M:%S')\n",
        "tweetsT = tweets['created_at']\n",
        "\n",
        "trace = go.Histogram(\n",
        "    x=tweetsT,\n",
        "    marker=dict(\n",
        "        color='blue'\n",
        "    ),\n",
        "    opacity=0.75\n",
        ")\n",
        "\n",
        "layout = go.Layout(\n",
        "    title='Aktivitas tweet sepanjang waktu',\n",
        "    height=450,\n",
        "    width=1200,\n",
        "    xaxis=dict(\n",
        "        title='Bulan dan tahun'\n",
        "    ),\n",
        "    yaxis=dict(\n",
        "        title='Kuantitas tweet'\n",
        "    ),\n",
        "    bargap=0.2,\n",
        ")\n",
        "\n",
        "data = [trace]\n",
        "\n",
        "fig = go.Figure(data=data, layout=layout)\n",
        "py.offline.iplot(fig)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hkUd2295rrNK"
      },
      "source": [
        "## <b>3. Pra-pemrosesan data</b><br>\n",
        "Data yang berasal dari Twitter, atau lebih tepatnya adalah data yang berupa teks adalah data bersifat tidak terstruktur dalam artian data tersebut memiliki format penulisan bebas (tidak sesuai kaidah penulisan yang benar, misalnya kaidah penulisan ejaan baku bahasa indonesia). Oleh karena itu, perlu untuk dilakukan pra-pemrosesan data untuk melakukan normalisasi isi <i>tweet</i>. \n",
        "\n",
        "Dalam percobaan ini, dilakukan pra-pemrosesan berupa:\n",
        "\n",
        "    a. Pembersihan tweet\n",
        "    Adapun yang dilakukan untuik membersihkan tweet: lowercasing; menghapus url, string b' & RT, emoji, url, username, hashtag, tanda baca, angka, spasi berlebih, trimming, pembuangan tanda baca, dan huruf berulang. Fungsi cleaning(tweet) menerima input berupa satu buah tweet mentah bertipe string. \n",
        "\n",
        "<b>#Code 3a</b><br>\n",
        "<i>Code</i> ini digunakan untuk pembersihan dari setiap tweet yang akan diolah, hal ini dimaksudkan agar mendapatkan data tweet yang lebih \"bersih\" dan lebih mudah untuk dilakukan pengolahan lebih lanjut"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8yRODX-jrtXr"
      },
      "outputs": [],
      "source": [
        "def clean_data(tweet):\n",
        "    # lowercase\n",
        "    normal_tw = tweet.lower()\n",
        "    # hapus b'\n",
        "    normal_tw = re.sub(r'^b\\'', '', normal_tw)\n",
        "    # hapus RT\n",
        "    normal_tw = re.sub(r'^rt ', '', normal_tw)\n",
        "    # hapus emoji\n",
        "    normal_tw = re.sub(r'\\\\x.{2}', '', normal_tw)\n",
        "    # hapus www.* atau https?://* (URL)\n",
        "    normal_tw = re.sub(r'((www\\.[^\\s]*)|(https?://[^\\s]*))', '', normal_tw)\n",
        "    # remove spasi berlebih\n",
        "    normal_tw = re.sub(r'\\s+', ' ', normal_tw)\n",
        "    # trim depan belakang\n",
        "    normal_tw = normal_tw.strip()\n",
        "    # regex huruf yang berulang kaya haiiii (untuk fitur unigram)\n",
        "    normal_regex = re.compile(r\"(.)\\1{1,}\")\n",
        "    # buang huruf yang berulang\n",
        "    normal_tw = normal_regex.sub(r\"\\1\\1\", normal_tw)\n",
        "    # hapus @username\n",
        "    normal_tw = re.sub(r'@[^\\s]+', '', normal_tw)\n",
        "    # hapus hashtag\n",
        "    normal_tw = re.sub(r'#[^\\s]+', '', normal_tw)\n",
        "    # hapus tanda baca\n",
        "    normal_tw = re.sub(r'[^\\w\\s]', '', normal_tw) \n",
        "    # hapus angka\n",
        "    normal_tw = re.sub(r'\\d+', ' ', normal_tw) \n",
        "    return normal_tw"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GWlltU4Jru0E"
      },
      "source": [
        "    b. Pembuangan stopwords dan normalisasi\n",
        "    Daftar stopwords didapatkan dari penelitian Tala:\n",
        "    Tala, F. Z. (2003). A Study of Stemming Effects on Information Retrieval in Bahasa Indonesia. M.S. thesis. M.Sc. Thesis. Master of Logic Project. Institute for Logic, Language and Computation. Universiteti van Amsterdam The Netherlands.\n",
        "    \n",
        "    Kamus singkatan didapatkan dari penelitian Hakim:\n",
        "    Hakim, A. N. (2016). Pemrosesan Pertanyaan pada Sistem Tanya Jawab Bidang Kesehatan dengan Pendekatan Pembelajaran Mesin. Bachelorâ€™s Thesis, Universitas Indonesia, Kampus UI Depok.\n",
        "    \n",
        "    Kamus alay didapatkan dari penelitian Salsabila:\n",
        "    N. Aliyah Salsabila, Y. Ardhito Winatmoko, A. Akbar Septiandri and A. Jamal, \"Colloquial Indonesian Lexicon,\" 2018 International Conference on Asian Language Processing (IALP), Bandung, Indonesia, 2018, pp. 226-229, doi: 10.1109/IALP.2018.8629151.\n",
        "\n",
        "    Fungsi remove_stopwords_and_normalize(tweet) menerima masukan berupa tweet yang telah dibersihkan yang bertipe string.\n",
        "    \n",
        "<b># Code 3b</b><br>\n",
        "<i>Code</i> ini digunakan untuk penghapusan stopwords dan normalisasi untuk setiap tweet yang akan diolah"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SwrPiRNFrxiG"
      },
      "outputs": [],
      "source": [
        "def remove_stopwords_and_normalize(tweet):\n",
        "    token = nltk.word_tokenize(tweet)\n",
        "    token_new = []\n",
        "    for k in token:\n",
        "        if k in df_kamus_singkatan['singkatan'].values:\n",
        "            k = df_kamus_singkatan.loc[df_kamus_singkatan['singkatan']\n",
        "                                       == k, 'asli'].values[0]\n",
        "        if k in df_kamus_alay['slang'].values:\n",
        "            k = df_kamus_alay.loc[df_kamus_alay['slang']\n",
        "                                  == k, 'formal'].values[0]\n",
        "        if k not in stopwords[0].values:\n",
        "            token_new.append(k)\n",
        "\n",
        "    str_clean = ' '.join(token_new)\n",
        "    return str_clean"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ewWcNXperyl0"
      },
      "source": [
        "    c. Pra-pemrosesan tweet secara keseluruhan\n",
        "    Pada tahap ini, akan dilakukan pemanggilan fungsi clean_data dan remove_stopwords_and_normalize yang sudah didefinisikan sebelumnya. Setelah itu akan ditampilkan sampel tiga tweet pertama hasil pra-pemrosesan.\n",
        "    \n",
        "<b>#Code 3c</b><br>\n",
        "<i>Code</i> ini digunakan untuk memanggil fungsi - fungsi pra pemrosean data yang sebelumnya sudah didefinisikan"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AQVaE1_1rzrE"
      },
      "outputs": [],
      "source": [
        "def pra_pemrosesan(list_tweet):\n",
        "    tweet_clean = []\n",
        "    for tw in tqdm(list_tweet):\n",
        "        normal_tweet = clean_data(tw)\n",
        "        normal_tweet = remove_stopwords_and_normalize(normal_tweet)\n",
        "        tweet_clean.append(normal_tweet)\n",
        "    return tweet_clean"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hntuQwbfr0pC"
      },
      "outputs": [],
      "source": [
        "raw_tweet = tweets['text']\n",
        "\n",
        "stopwords = pd.read_csv(\"stopwordsID.csv\", header=None)\n",
        "df_kamus_singkatan = pd.read_csv('kamus_singkatan.csv')\n",
        "df_kamus_alay = pd.read_csv('colloquial-indonesian-lexicon.csv')\n",
        "\n",
        "# Melakukan pra pemrosesan tweet.\n",
        "# CATATAN: proses ini akan memakan waktu sekitar 15-20 menit karena banyaknya data yang perlu diproses\n",
        "print(\"Sedang memproses tweet... Mohon menunggu\")\n",
        "clean_tweet = pra_pemrosesan(raw_tweet)\n",
        "\n",
        "# Menampilkan 10 tweet pertama yang telah dibersihkan\n",
        "clean_tweet[:10]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hWjiGyotr2Dq"
      },
      "source": [
        "Jika Anda menemukan bahwa hasil pra pemrosesan belum sempurna hingga menjadi sekumpulan tweet yang mengikuti ejaan bahasa indonesia baku, hal tersebut bukanlah masalah, sebab hingga saat ini memang belum ada cara yang sempurna untuk melakukan pembersihan dan pengolahan awal pada teks bahasa indonesia"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qfT-Gyh5r6V2"
      },
      "source": [
        "## <b>4. Membuat korpus dari seluruh data <i>tweet</i></b><br>\n",
        "Seluruh teks dalam dokumen <i>tweet</i> digabungkan menjadi korpus.\n",
        "\n",
        "\n",
        "<b>#Code 4</b><br>\n",
        "<i>Code</i> ini digunakan untuk membuat korpus yang dibentuk dari semua data tweet yang sudah dilakukan pra pemrosesan"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H9XqDfr5r7Zz"
      },
      "outputs": [],
      "source": [
        "def create_tweet_corpus(list_tweet):\n",
        "    corpus = []\n",
        "    for tweet in list_tweet:\n",
        "        if not pd.isnull(tweet):\n",
        "            words = nltk.word_tokenize(tweet)\n",
        "            for word in words:\n",
        "                corpus.append([word])\n",
        "    return corpus\n",
        "\n",
        "tweet_corpus = create_tweet_corpus(clean_tweet)\n",
        "tweet_corpus[:10]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "54Vcgamhr9G8"
      },
      "source": [
        "## <b> 5. Membuat <i>term dictionary</i></b><br>\n",
        "Membuat <i>term dictionary</i> dari korpus yang telah dibuat. Setiap <i>term</i> unik diberikan suatu indeks.\n",
        "    \n",
        "<b>#Code 5</b><br>\n",
        "<i>Code</i> ini digunakan untuk membuat dictionary dari korpus, dimana setiap kata unik dipetakan dengan indeks. Pembuatannya sendiri menggunakan library gensim."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "52yrxW74r_QK"
      },
      "outputs": [],
      "source": [
        "dictionary = corpora.Dictionary(tweet_corpus)\n",
        "\n",
        "# Mencoba melihat 5 indeks pertama isi dari term dictionary\n",
        "for index in range(10):\n",
        "    print(dictionary.get(index))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u7QJ__njsAhs"
      },
      "source": [
        "Bisa dilihat pada output dari kode diatas merupakan hasil pemetaan dari kata - kata yang muncul pada #Code 4 dengan indeks yang dimulai dari 0 hingga N, dimana N adalah banyaknya kata / term. Meskipun begitu, term dictionary dengan library gensim sudah mengolah kata yang menjadi duplikat, sehingga kata yang duplikat tetap berada di indeks yang sama dan tidak dimasukkan dalam indeks yang berbeda.\n",
        "\n",
        "Contoh:  \n",
        "Kalimat <b>asyik libur libur libur</b> jika dipetakan menjadi {0: \"asyik\", 1: \"libur\"} dan bukan menjadi {0: \"asyik\", 1: \"libur\", 2: \"libur\", 3: \"libur\"}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0ImC6UxssBow"
      },
      "source": [
        "## <b> 6. Membuat <i>term document frequency</i></b><br>\n",
        "Membuat <i>term document frequency</i> dari korpus menggunakan <i>dictionary</i> yang sudah didefinisikan sebelumnya. \n",
        "\n",
        "<b>#Code 6</b><br>\n",
        "<i>Code</i> ini digunakan untuk membuat term document frequency. Berbeda dengan term frequency, pemetaan yang dilakukan adalah pemetaan indeks dengan frekuensi suatu kata pada dokumen.  \n",
        "  \n",
        "Contoh:  \n",
        "Kalimat <b>asyik libur libur libur</b> terdiri dari \"asyik\" dengan frekuensi 1 kata, dan \"libur\" dengan frekuensi 3 kata. Dengan demikian term document frequency untuk kalimat tersebut adalah [(0,1),(1,3)] dimana setiap elemen pada array terbut adalah berupa tuple (indeks, frekuensi suatu kata X pada data)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CTg2GbLasDVE"
      },
      "outputs": [],
      "source": [
        "# Term Document Frequency: converting list of documents (corpus) into Document Term Matrix using dictionary prepared above.\n",
        "corpus = [dictionary.doc2bow(text) for text in tweet_corpus]\n",
        "\n",
        "# Menampilkan 10 data pertama\n",
        "print(corpus[:10])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KKvmlJvwsFfZ"
      },
      "source": [
        "## <b>7. Pembentukan model & <i>training</i> data menggunakan LDA</b><br>\n",
        "Melakukan <i>training</i> menggunakan <i>corpus term document frequency</i>. Total topik yang dipilih sebanyak 5. Anda bisa bereksperimen dengan mengganti jumlah total topik dalam melakukan <i>training</i> pada model. Kemudian ditampilkan 10 kata yang paling penting pada setiap topik yang terbentuk pada model.\n",
        "\n",
        "<b>#Code 7</b><br>\n",
        "<i>Code</i> ini digunakan untuk membangun model LDA serta melakukan training data menggunakan model LDA yang sudah dibangun."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n69zOthysGrs"
      },
      "outputs": [],
      "source": [
        "tfidf = models.TfidfModel(corpus)  # Tahap 1 -- menginisialisasi model\n",
        "corpus_tfidf = tfidf[corpus]  # Tahap 2 -- menggunakan model untuk mendapatkan corpus vectors\n",
        "\n",
        "total_topics = 5 # Jumlah topik = 5\n",
        "lda = models.ldamodel.LdaModel(corpus, id2word=dictionary, num_topics=total_topics)\n",
        "corpus_lda = lda[corpus]\n",
        "\n",
        "# Menampilkan 10 kata paling penting pertama di masing-masing topik:\n",
        "lda.show_topics(total_topics, 10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UbYHJQD5sICJ"
      },
      "source": [
        "Dari setiap topik yang terbentuk, Anda bisa melihat kata-kata apa saja yang banyak muncul dari topik tersebut. Kemudian Anda bisa menggeneralisir topik yang dibicarakan pada klaster tersebut."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "42kzEqHisJ6p"
      },
      "source": [
        "## <b>8. Visualisasi</b><br>\n",
        "Dalam visualisasi ini, Anda bisa melihat persebaran kata pada setiap topik yang sudah dibentuk\n",
        "\n",
        "<b>#Code 8a</b><br>\n",
        "<i>Code</i> ini digunakan untuk menampilkan visualisasi dari model LDA dengan beberapa topic"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6kUI7570sK_L"
      },
      "outputs": [],
      "source": [
        "pyLDAvis.enable_notebook()\n",
        "panel = pyLDAvis.gensim_models.prepare(lda, corpus, dictionary, mds='tsne')\n",
        "panel"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CG1yD3bLrRKc"
      },
      "source": [
        "<b>#Code 8b</b><br>\n",
        "<i>Code</i> ini digunakan untuk membuat wordcloud. Word cloud secara umum berisi kata - kata yang muncul dalam data, yang dipetakan berdasarkan frekuensi. Semakin besar ukuran huruf suatu kata pada wordcloud, maka kata tersebut semakin sering muncul pada kumpulan data. Berikut ini adalah wordcloud yang dihasilkan dari semua tweet yang sudah dilakukan preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pNR-P_N4rRSd"
      },
      "outputs": [],
      "source": [
        "long_string = ';'.join(clean_tweet)\n",
        "wordcloud = WordCloud(background_color='white', max_words=5000, \n",
        "                      contour_width=3, contour_color='steelblue')\n",
        "wordcloud.generate(long_string)\n",
        "wordcloud.to_image()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0z3HuvtJa7N7"
      },
      "source": [
        "## <b>9. Evaluasi</b><br>\n",
        "Evaluasi merupakan langkah yang penting agar dapat mengetahui bagaimana kualitas dari topic modelling yang telah dibangun. Pada bagian ini akan dilakukan evaluasi model yang dibentuk menggunakan LDA dengan meghitung nilai koherensi nya (Coherence) dimana semakin tinggi nilainya maka semakin baik juga hasil topic modelling yang telah dibangun.  \n",
        "Evaluasi menggunakan dua jenis skor, Perplexity dan Coherence. \n",
        "\n",
        "<b>#Code 9</b><br>\n",
        "<i>Code</i> ini digunakan untuk melakukan evaluasi LDA dengan membentuk fungsi - fungsi yang bersifat reusable"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UziMcq9WIDXi"
      },
      "outputs": [],
      "source": [
        "def calculate_perplexity(corpus):\n",
        "    return lda.log_perplexity(corpus)\n",
        "\n",
        "def calculate_coherence(model, word_corpus, dictionary):\n",
        "    coherence_model_lda = CoherenceModel(model=model, texts=word_corpus, dictionary=dictionary, coherence='c_v')\n",
        "    return coherence_model_lda.get_coherence()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ifvnX2tNBW1z"
      },
      "outputs": [],
      "source": [
        "# Menghitung nilai perplexity untuk mengetahui seberapa baik model yang dibangun, semakin kecil nilainya maka semakin baik kualitasnya\n",
        "print('\\nNilai Perplexity: ', calculate_perplexity(corpus))\n",
        "\n",
        "# Menghitung nilai koherensi / coherence. Rentang nilai berada di 1 < x < 0. Nilai yang bagus biasanya berada di rentang 0,85 < x < 0,65\n",
        "print('\\nNilai Coherence: ', calculate_coherence(lda, tweet_corpus, dictionary))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IVbro3cJCoX1"
      },
      "source": [
        "Apakah nilai tersebut sudah mencerminkan bahwa hasil topic modelling yang dirancang sudah baik ? Belum tentu, karena untuk mendapatkan hasil yang lebih baik, salah satu faktor yang dapat diperbaiki adalah menentukan jumlah topic yang akan dipetakan. Saat ini hanya menggunakan 5 topic (k=5), namun nilai topic bisa berbeda. Lantas bagaimana menentukan nilai yang lebih baik ? Akan dijelaskan dibawah ini"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JY2ML7DaDD8g"
      },
      "source": [
        "### <b>Mencari jumlah topic yang optimal untuk LDA</b>\n",
        "Pendekatan yang digunakan disini adalah melakukan iterasi nilai K (nilai besarnya angka topic) dari rentang inklusif [x,y] kemudian untuk setiap nilai K, hitung nilai perplexity dan coherencenya."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WKo8SCXFNUL2"
      },
      "source": [
        "<b>#Code 9.1a</b><br>\n",
        "<i>Code</i> ini berisi fungsi yang akan digunakan untuk mencari nilai topic terbaik dan melakukan visualisasi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G6m4rjVPDCJ1"
      },
      "outputs": [],
      "source": [
        "def compute_coherence_values(dictionary, corpus, texts, limit, start, step):\n",
        "    \"\"\"\n",
        "    Fungsi menghitung coherence (c_v) untuk berbagai nilai topic.\n",
        "\n",
        "    Parameter:\n",
        "    ----------\n",
        "    dictionary : Gensim dictionary\n",
        "    corpus : Korpus data asli\n",
        "    texts : Korpus tweet yang berada di #Code 4\n",
        "    start : nilai awal topic\n",
        "    limit : batas nilai topic\n",
        "    step : increment nilai topic\n",
        "\n",
        "    Pada contoh sebelumnya, nilai topic adalah 5, namun sekarang nilai topic\n",
        "    di-iterasi dari rentang [start, limit] dengan incremental sebesar 'step'\n",
        "\n",
        "    Keluaran:\n",
        "    -------\n",
        "    model_list : List dari LDA topic modelling yang bersesuaian dengan nilai topic tertentu\n",
        "    coherence_values : Nilai coherence dari LDA topic modelling yang bersesuaian dengan nilai topic tertentu\n",
        "    \"\"\"\n",
        "    model_list = []\n",
        "    coherence_values = []\n",
        "    perplexity_values = []\n",
        "    for num_topics in range(start, limit, step):\n",
        "        print(\"Saat ini mengolah nilai topic = \", num_topics)\n",
        "        # Menambahkan model - model LDA\n",
        "        model = models.ldamodel.LdaModel(corpus, id2word=dictionary, num_topics=num_topics)\n",
        "        model_list.append(model)\n",
        "        # Menambahkan nilai - nilai coherence\n",
        "        coherencemodel = calculate_coherence(model, texts, dictionary)\n",
        "        coherence_values.append(coherencemodel)\n",
        "        # Menambahkan nilai - nilai perplexity\n",
        "        perplexityvalue = calculate_perplexity(corpus)\n",
        "        perplexity_values.append(perplexityvalue)\n",
        "    return model_list, coherence_values, perplexity_values\n",
        "\n",
        "def visualize_perplexity(perplexity_values, start, limit, step):\n",
        "    x = range(start, limit, step)\n",
        "    plt.plot(x, perplexity_values)\n",
        "    plt.xlabel(\"Besar nilai perplexity\")\n",
        "    plt.ylabel(\"Nilai perplexity\")\n",
        "    plt.legend((\"perplexity_values\"), loc='best')\n",
        "    plt.show()\n",
        "\n",
        "def visualize_coherence(coherence_values, start, limit, step):\n",
        "    x = range(start, limit, step)\n",
        "    plt.plot(x, coherence_values)\n",
        "    plt.xlabel(\"Besar nilai topic\")\n",
        "    plt.ylabel(\"Nilai coherence\")\n",
        "    plt.legend((\"coherence_values\"), loc='best')\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4sChKkYpSmu_"
      },
      "source": [
        "<b>#Code 9.1b</b><br>\n",
        "<i>Code</i> ini berfungsi untuk menjalankan pemanggilan fungsi untuk mendapatkan daftar model, daftar nilai coherence, dan daftar nilai perplexity"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "diZ5jN5xEcP4"
      },
      "outputs": [],
      "source": [
        "# PERHATIAN: Proses ini bisa memakan waktu 10-30 menit\n",
        "TOPIC_START = 1\n",
        "TOPIC_LIMIT = 13\n",
        "TOPIC_STEP = 5\n",
        "\n",
        "# Pembentukan daftar nilai model dan nilai coherence nya\n",
        "model_list, coherence_values, perplexity_values = compute_coherence_values(\n",
        "    dictionary=dictionary,\n",
        "    corpus=corpus,\n",
        "    texts=tweet_corpus,\n",
        "    start=TOPIC_START,\n",
        "    limit=TOPIC_LIMIT,\n",
        "    step=TOPIC_STEP\n",
        ")\n",
        "\n",
        "# Visualisasi nilai perplexity\n",
        "visualize_perplexity(perplexity_values, TOPIC_START, TOPIC_LIMIT, TOPIC_STEP)\n",
        "\n",
        "# Visualisasi nilai topic\n",
        "visualize_coherence(coherence_values, TOPIC_START, TOPIC_LIMIT, TOPIC_STEP)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oARtzzgtbP5Y"
      },
      "source": [
        "## <b>10. Topic Modelling dengan Metode Lain</b><br>\n",
        "Pada self pace learning kali ini akan menggunakan K-Means Clustering dan NNMF (Non Negative Matrix Factorization)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gfDgHEAysN8h"
      },
      "source": [
        "### <b>1. Pemodelan topik menggunakan K-Means Clustering</b><br>\n",
        "Salah satu algoritma <i>clustering</i> lainnya yang umum digunakan adalah K-Means. Berikut adalah kode untuk mengeksekusi algoritma K-Means menggunakan library `sklearn`. Anda dapat membandingkan hasil <i>clustering</i> yang dilakukan oleh LDA dengan K-Means.\n",
        "\n",
        "<b>#Code 10.1a</b><br>\n",
        "<i>Code</i> ini digunakan untuk membangun model K-Means Clustering dengan nilai K=5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "RT2y22AysPSr"
      },
      "outputs": [],
      "source": [
        "vectorizer = TfidfVectorizer()\n",
        "X = vectorizer.fit_transform(clean_tweet)\n",
        "\n",
        "# Train algoritma K-Means\n",
        "kmeans = KMeans(n_clusters=5) # jumlah klaster = 5\n",
        "kmeans.fit(X)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ljSgZZdIsREA"
      },
      "source": [
        "<b>#Code 10.1b</b><br>\n",
        "<i>Code</i> ini digunakan menampilkan beberapa hasil kata dari cluster yang sudah dibangun"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "TjoZhtIMsR-3"
      },
      "outputs": [],
      "source": [
        "# Membuat dictionary untuk data cluster\n",
        "clusters = {}\n",
        "\n",
        "for tweet, cluster in zip(clean_tweet, kmeans.labels_.tolist()):\n",
        "    if cluster not in clusters:\n",
        "        clusters[cluster] = [tweet]\n",
        "    else:\n",
        "        clusters[cluster].append(tweet)\n",
        "\n",
        "# Menampilkan sejumlah n=20 kata-kata dominan di masing-masing cluster\n",
        "n_words = 20\n",
        "for cluster, tweets in clusters.items():\n",
        "    long_string = \" \".join(tweets)\n",
        "    words = nltk.word_tokenize(long_string)\n",
        "    dominant_words = pd.Series(words).value_counts()[:n_words].index.to_list()\n",
        "\n",
        "    print(\"Cluster \" + str(cluster))\n",
        "    print(\" | \".join(dominant_words))\n",
        "    print('='*90)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d2OJazPPaRr9"
      },
      "source": [
        "### <b>2. Pemodelan topik menggunakan NNMF</b><br>\n",
        "\n",
        "NNMF atau yang biasa disebut dengan Non-Negative Matrix Factorization sejatinya adalah metode statistik yang digunakan untuk menurunkan dimensi dari input yang berupa korpus. NNMF menggunakan metode analisis faktor untuk memberikan pemetaan kata - kata yang memiliki koherensi rendah dengan kata yang berbobot rendah. Cara kerja NNMF adalah dengan melakukan <i>decompose</i> vektor berdimensi tinggi men jadi vektor berdimensi rendah. Dengan menggunakan sebuah matriks (A), NNMF akan menghasilkan dua buah matriks (B dan C) dimana matriks X berisi topic yang ditemukan, dan matriks Y berisi bobot dari topik - topik tersebut.\n",
        "\n",
        "Asumsikan ada 400  (Baris data), 5000 kata (Total) dan 50 topik, maka akan menghasilkan matriks berikut:\n",
        "- A = 400 x 5000\n",
        "- B = 5000 x 50\n",
        "- C = 50 x 400\n",
        "\n",
        "<b>#Code 10.2a</b><br>\n",
        "<i>Code</i> ini digunakan untuk melakukan transformasi input data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "une-zbctaRsE"
      },
      "outputs": [],
      "source": [
        "vectorizer = TfidfVectorizer()\n",
        "X = vectorizer.fit_transform(clean_tweet)\n",
        "words = np.array(vectorizer.get_feature_names())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1fEY8a-PaRsE"
      },
      "source": [
        "<b>#Code 10.2b</b><br>\n",
        "<i>Code</i> ini digunakan menampilkan beberapa topic yang sudah dibangun"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "JUp4V0D3k3FU"
      },
      "outputs": [],
      "source": [
        "nmf = NMF(n_components=5, solver=\"mu\")\n",
        "B = nmf.fit_transform(X)\n",
        "C = nmf.components_\n",
        "\n",
        "for i, topic in enumerate(C):\n",
        "     print(\"Topic {}: {}\".format(i + 1, \",\".join([str(x) for x in words[topic.argsort()[-10:]]])))\n",
        "\n",
        "print(\"\")\n",
        "print(\"Matriks B ada sebanyak \" + str(len(B)) + \" baris dan \" + str(len(B[0])) + \" kolom\")\n",
        "print(\"Matriks C ada sebanyak \" + str(len(C)) + \" baris dan \" + str(len(C[0])) + \" kolom\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PqLQrFiosTTy"
      },
      "source": [
        "# <b><font color='red'>B. Tugas Self Paced Learning - Topic Modelling</font></b>\n",
        "\n",
        "1. <b>[10 Poin]</b> Terlihat pada hasil keluaran kode pada cell #Code2a tanggal 24 Februari 2022 dan 4 Maret 2022 terjadi lonjakan aktivitas yang cukup tajam. Menurut Anda berdasarkan ketiga keyword utama yang berperan membangun dataset ini (Refer ke Section 2), keyword manakah yang menyebabkan adanya lonjakan aktivitas pada tanggal tersebut (Keyword 1/2/3) ? dan berikan alasannya.\n",
        "\n",
        "2. <b>[10 Poin]</b> Menurut Anda, apa tujuan utama dari <i>pembentukan term document dan term document frequency</i> ? Apakah ada keuntungan dan kerugian yang mungkin diakibatkan dari proses ini apabila teknik ini tidak hanya diterapkan pada topic modelling, tapi pada aktivitas lain yang membutuhkan manipulasi data tekstual ?\n",
        "\n",
        "3. <b>[10 Poin]</b> Berdasarkan hasil dari <b>#Code 7</b> apa maksud dari angka desimal yang muncul disamping setiap kata pada hasil keluaran kode? Berikan penjelasan Anda\n",
        "\n",
        "4. <b>[30 Poin]</b> Cobalah untuk menjalankan tahap <b>4. Membuat korpus dari seluruh data tweet</b> hingga akhir tetapi menggunakan data tweet indeks inklusif [0,10000] , [20000,30000], dan [30000,40000] dalam percobaan terpisah, dengan begitu akan ada 3x percobaan). Apakah masing - masing penggolongan topic yang dihasilkan lebih baik (berdasarkan pengetahuan manusia) daripada apa yang dihasilkan di <b>#Code 8a</b> ? Jelaskan jawaban Anda\n",
        "\n",
        "Catatan nomor 4, perbandingan yang dimaksud adalah\n",
        "- Data indeks [10,20000] vs hasil dari <b>#Code 8a</b>\n",
        "- Data indeks [20000,30000] vs hasil dari <b>#Code 8a</b>\n",
        "- Data indeks [30000,akhir] vs hasil dari<b>#Code 8a</b>\n",
        "\n",
        "5. <b>[40 Poin]</b> Anda diberikan dataset baru dengan keyword tertentu yang diambil dari tanggal 1 Januari 2021 hingga 31 Desember 2021 dengan maksimal 11000 tweets. Silakan lakukan semua tahap yang sudah diajarkan pada self paced learning ini dari mulai tahap preprocessing hingga pembentukan visualisasi dari topic modelling (awal hingga akhir). Jelaskan setiap tahap yang dilakukan dan jelaskan juga apa hasil serta kesimpulan atau hal menarik apa yang Anda dapatkan!\n",
        "\n",
        "Ketentuan dataset:\n",
        "Dataset sudah disediakan di cell bawah, cukup gunakan kolom `id`, `text` dan `created_at` saja\n",
        "- Laki - laki dengan angka terakhir NPM GANJIL: keyword `menguasai dunia`\n",
        "- Laki - laki dengan angka terakhir NPM GENAP: keyword `butuh liburan`\n",
        "- Perempuan dengan angka terakhir NPM GANJIL: keyword `makan seblak`\n",
        "- Perempuan dengan angka terakhir NPM GENAP: keyword `nonton kpop`\n",
        "\n",
        "Jawaban nomor 5 harus mengandung:\n",
        "- Preprosessing\n",
        "- Pembuatan korpus\n",
        "- Membuat term dictionary & term document frequency\n",
        "- Pembentukan model dengan LDA\n",
        "- Visualisasi\n",
        "- Evaluasi\n",
        "\n",
        "Selamat Mengerjakan!  \n",
        "Jangan lupa berdoa, makan dan mencuci tangan!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JjE1vhg-EaD8"
      },
      "outputs": [],
      "source": [
        "# Dataset keyword \"menguasai dunia\", silakan di-uncomment semua baris di bawah ini\n",
        "\n",
        "# !wget -q --no-check-certificate 'https://docs.google.com/uc?export=download&id=1p2BMF7GDZA6u3wmvbxMO5bs3QxU4m8A_' -O menguasai_dunia.csv\n",
        "# datafile_a = 'menguasai_dunia.csv'\n",
        "# tweets_a = pd.read_csv(datafile_a)\n",
        "# tweets_a"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cHbo893pEaMQ"
      },
      "outputs": [],
      "source": [
        "# Dataset keyword \"butuh liburan\", silakan di-uncomment semua baris di bawah ini\n",
        "\n",
        "# !wget -q --no-check-certificate 'https://docs.google.com/uc?export=download&id=1aGY6y9vABCKD08MkA6Rwv2FkALHf5VbY' -O butuh_liburan.csv\n",
        "# datafile_b = 'butuh_liburan.csv'\n",
        "# tweets_b = pd.read_csv(datafile_b)\n",
        "# tweets_b"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Va6hpUo2EaUC"
      },
      "outputs": [],
      "source": [
        "# Dataset keyword \"makan seblak\", silakan di-uncomment semua baris di bawah ini\n",
        "\n",
        "# !wget -q --no-check-certificate 'https://docs.google.com/uc?export=download&id=1x-8ev32dumbu0zENIrEtIJQPfmqKk6H-' -O makan_seblak.csv\n",
        "# datafile_c = 'makan_seblak.csv'\n",
        "# tweets_c = pd.read_csv(datafile_c)\n",
        "# tweets_c"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2H4Om2tuEaZ_"
      },
      "outputs": [],
      "source": [
        "# Dataset keyword \"nonton kpop\", silakan di-uncomment semua baris di bawah ini\n",
        "\n",
        "# !wget -q --no-check-certificate 'https://docs.google.com/uc?export=download&id=1IzTzONy2Ih5YyXQoMYDd60Nh79YakmQ4' -O nonton_kpop.csv\n",
        "# datafile_d = 'nonton_kpop.csv'\n",
        "# tweets_d = pd.read_csv(datafile_d)\n",
        "# tweets_d"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "9YFT-C9LreQN",
        "VbgVpyTnrikR",
        "qfT-Gyh5r6V2",
        "54Vcgamhr9G8",
        "0ImC6UxssBow",
        "KKvmlJvwsFfZ",
        "42kzEqHisJ6p",
        "d2OJazPPaRr9",
        "PqLQrFiosTTy"
      ],
      "name": "Self Paced Learning Topic Modelling.ipynb",
      "toc_visible": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}