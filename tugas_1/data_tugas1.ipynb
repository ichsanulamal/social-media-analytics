{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Teks</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ID</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>R1</th>\n",
       "      <td>Penjualnya ramah dan pengiriman cepat kereeen</td>\n",
       "      <td>Positif</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>R2</th>\n",
       "      <td>dijual harga murah tapi tidak murahan, wajib d...</td>\n",
       "      <td>Positif</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>R3</th>\n",
       "      <td>SEBELLL!!!! mending cari penjual lain aja ??????</td>\n",
       "      <td>Negatif</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>R4</th>\n",
       "      <td>Jualannya murahan tapi harga mahal ????</td>\n",
       "      <td>Negatif</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>R5</th>\n",
       "      <td>Lama dikirimnya, padahal udah dari lama ordernya</td>\n",
       "      <td>Negatif</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 Teks    Label\n",
       "ID                                                            \n",
       "R1      Penjualnya ramah dan pengiriman cepat kereeen  Positif\n",
       "R2  dijual harga murah tapi tidak murahan, wajib d...  Positif\n",
       "R3   SEBELLL!!!! mending cari penjual lain aja ??????  Negatif\n",
       "R4            Jualannya murahan tapi harga mahal ????  Negatif\n",
       "R5   Lama dikirimnya, padahal udah dari lama ordernya  Negatif"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"data_tugas1.csv\", index_col=\"ID\")\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Case Folding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Case folding\n",
    "data['Teks'] = data['Teks'].str.lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string \n",
    "import re #regex library\n",
    "\n",
    "# import word_tokenize & FreqDist from NLTK\n",
    "from nltk.tokenize import word_tokenize \n",
    "from nltk.probability import FreqDist\n",
    "\n",
    "# ------ Tokenizing ---------\n",
    "\n",
    "def remove_sentence_special(text):\n",
    "    # remove tab, new line, ans back slice\n",
    "    text = text.replace('\\\\t',\" \").replace('\\\\n',\" \").replace('\\\\u',\" \").replace('\\\\',\"\")\n",
    "    # remove non ASCII (emoticon, chinese word, .etc)\n",
    "    text = text.encode('ascii', 'replace').decode('ascii')\n",
    "    # remove mention, link, hashtag\n",
    "    text = ' '.join(re.sub(\"([@#][A-Za-z0-9]+)|(\\w+:\\/\\/\\S+)\",\" \", text).split())\n",
    "    # remove incomplete URL\n",
    "    return text.replace(\"http://\", \" \").replace(\"https://\", \" \")\n",
    "                \n",
    "data['Teks'] = data['Teks'].apply(remove_sentence_special)\n",
    "\n",
    "#remove number\n",
    "def remove_number(text):\n",
    "    return  re.sub(r\"\\d+\", \"\", text)\n",
    "\n",
    "data['Teks'] = data['Teks'].apply(remove_number)\n",
    "\n",
    "#remove punctuation\n",
    "def remove_punctuation(text):\n",
    "    return text.translate(str.maketrans(\"\",\"\",string.punctuation))\n",
    "\n",
    "data['Teks'] = data['Teks'].apply(remove_punctuation)\n",
    "\n",
    "#remove whitespace leading & trailing\n",
    "def remove_whitespace_LT(text):\n",
    "    return text.strip()\n",
    "\n",
    "data['Teks'] = data['Teks'].apply(remove_whitespace_LT)\n",
    "\n",
    "#remove multiple whitespace into single whitespace\n",
    "def remove_whitespace_multiple(text):\n",
    "    return re.sub('\\s+',' ',text)\n",
    "\n",
    "data['Teks'] = data['Teks'].apply(remove_whitespace_multiple)\n",
    "\n",
    "# remove single char\n",
    "def remove_singl_char(text):\n",
    "    return re.sub(r\"\\b[a-zA-Z]\\b\", \"\", text)\n",
    "\n",
    "data['Teks'] = data['Teks'].apply(remove_singl_char)\n",
    "\n",
    "# NLTK word rokenize \n",
    "def word_tokenize_wrapper(text):\n",
    "    return word_tokenize(text)\n",
    "\n",
    "data['sentence_tokens'] = data['Teks'].apply(word_tokenize_wrapper)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "# ----------------------- get stopword from NLTK stopword -------------------------------\n",
    "# get stopword indonesia\n",
    "list_stopwords = stopwords.words('indonesian')\n",
    "\n",
    "\n",
    "# ---------------------------- manualy add stopword  ------------------------------------\n",
    "# append additional stopword\n",
    "list_stopwords.extend([\"yg\", \"dg\", \"rt\", \"dgn\", \"ny\", \"d\", 'klo', \n",
    "                       'kalo', 'amp', 'biar', 'bikin', 'bilang', \n",
    "                       'gak', 'ga', 'krn', 'nya', 'nih', 'sih', \n",
    "                       'si', 'tau', 'tdk', 'tuh', 'utk', 'ya', \n",
    "                       'jd', 'jgn', 'sdh', 'aja', 'n', 't', \n",
    "                       'nyg', 'hehe', 'pen', 'u', 'nan', 'loh', 'rt',\n",
    "                       '&amp', 'yah'])\n",
    "\n",
    "# ---------------------------------------------------------------------------------------\n",
    "\n",
    "# convert list to dictionary\n",
    "list_stopwords = set(list_stopwords)\n",
    "\n",
    "\n",
    "#remove stopword pada list token\n",
    "def stopwords_removal(words):\n",
    "    return [word for word in words if word not in list_stopwords]\n",
    "\n",
    "data['sentence_tokens_WSW'] = data['sentence_tokens'].apply(stopwords_removal) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Teks</th>\n",
       "      <th>Label</th>\n",
       "      <th>sentence_tokens</th>\n",
       "      <th>sentence_tokens_WSW</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ID</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>R1</th>\n",
       "      <td>penjualnya ramah dan pengiriman cepat kereeen</td>\n",
       "      <td>Positif</td>\n",
       "      <td>[penjualnya, ramah, dan, pengiriman, cepat, ke...</td>\n",
       "      <td>[penjualnya, ramah, pengiriman, cepat, kereeen]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>R2</th>\n",
       "      <td>dijual harga murah tapi tidak murahan wajib di...</td>\n",
       "      <td>Positif</td>\n",
       "      <td>[dijual, harga, murah, tapi, tidak, murahan, w...</td>\n",
       "      <td>[dijual, harga, murah, murahan, wajib, dibeli]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>R3</th>\n",
       "      <td>sebelll mending cari penjual lain aja</td>\n",
       "      <td>Negatif</td>\n",
       "      <td>[sebelll, mending, cari, penjual, lain, aja]</td>\n",
       "      <td>[sebelll, mending, cari, penjual]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>R4</th>\n",
       "      <td>jualannya murahan tapi harga mahal</td>\n",
       "      <td>Negatif</td>\n",
       "      <td>[jualannya, murahan, tapi, harga, mahal]</td>\n",
       "      <td>[jualannya, murahan, harga, mahal]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>R5</th>\n",
       "      <td>lama dikirimnya padahal udah dari lama ordernya</td>\n",
       "      <td>Negatif</td>\n",
       "      <td>[lama, dikirimnya, padahal, udah, dari, lama, ...</td>\n",
       "      <td>[dikirimnya, udah, ordernya]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 Teks    Label  \\\n",
       "ID                                                               \n",
       "R1      penjualnya ramah dan pengiriman cepat kereeen  Positif   \n",
       "R2  dijual harga murah tapi tidak murahan wajib di...  Positif   \n",
       "R3              sebelll mending cari penjual lain aja  Negatif   \n",
       "R4                 jualannya murahan tapi harga mahal  Negatif   \n",
       "R5    lama dikirimnya padahal udah dari lama ordernya  Negatif   \n",
       "\n",
       "                                      sentence_tokens  \\\n",
       "ID                                                      \n",
       "R1  [penjualnya, ramah, dan, pengiriman, cepat, ke...   \n",
       "R2  [dijual, harga, murah, tapi, tidak, murahan, w...   \n",
       "R3       [sebelll, mending, cari, penjual, lain, aja]   \n",
       "R4           [jualannya, murahan, tapi, harga, mahal]   \n",
       "R5  [lama, dikirimnya, padahal, udah, dari, lama, ...   \n",
       "\n",
       "                                sentence_tokens_WSW  \n",
       "ID                                                   \n",
       "R1  [penjualnya, ramah, pengiriman, cepat, kereeen]  \n",
       "R2   [dijual, harga, murah, murahan, wajib, dibeli]  \n",
       "R3                [sebelll, mending, cari, penjual]  \n",
       "R4               [jualannya, murahan, harga, mahal]  \n",
       "R5                     [dikirimnya, udah, ordernya]  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ID\n",
       "R1     [penjualnya, ramah, pengiriman, cepat, keren]\n",
       "R2    [dijual, harga, murah, murahan, wajib, dibeli]\n",
       "R3                   [sebel, mending, cari, penjual]\n",
       "R4                [jualannya, murahan, harga, mahal]\n",
       "R5                      [dikirimnya, udah, ordernya]\n",
       "Name: data_normalized, dtype: object"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normalizad_word_dict = {}\n",
    "normalizad_word_dict[\"kereeen\"] = \"keren\"\n",
    "normalizad_word_dict[\"sebelll\"] = \"sebel\"\n",
    "\n",
    "def normalized_term(document):\n",
    "    return [normalizad_word_dict[term] if term in normalizad_word_dict else term for term in document]\n",
    "\n",
    "data['data_normalized'] = data['sentence_tokens_WSW'].apply(normalized_term)\n",
    "\n",
    "data['data_normalized']\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20\n",
      "------------------------\n",
      "penjualnya : jual\n",
      "pengiriman : kirim\n",
      "dijual : jual\n",
      "murahan : murah\n",
      "dibeli : beli\n",
      "penjual : jual\n",
      "jualannya : jual\n",
      "dikirimnya : kirim\n",
      "ordernya : order\n",
      "{'penjualnya': 'jual', 'ramah': 'ramah', 'pengiriman': 'kirim', 'cepat': 'cepat', 'keren': 'keren', 'dijual': 'jual', 'harga': 'harga', 'murah': 'murah', 'murahan': 'murah', 'wajib': 'wajib', 'dibeli': 'beli', 'sebel': 'sebel', 'mending': 'mending', 'cari': 'cari', 'penjual': 'jual', 'jualannya': 'jual', 'mahal': 'mahal', 'dikirimnya': 'kirim', 'udah': 'udah', 'ordernya': 'order'}\n",
      "------------------------\n",
      "ID\n",
      "R1          [jual, ramah, kirim, cepat, keren]\n",
      "R2    [jual, harga, murah, murah, wajib, beli]\n",
      "R3                [sebel, mending, cari, jual]\n",
      "R4                 [jual, murah, harga, mahal]\n",
      "R5                        [kirim, udah, order]\n",
      "Name: data_tokens_stemmed, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# import Sastrawi package\n",
    "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
    "\n",
    "# create stemmer\n",
    "factory = StemmerFactory()\n",
    "stemmer = factory.create_stemmer()\n",
    "\n",
    "# stemmed\n",
    "def stemmed_wrapper(term):\n",
    "    return stemmer.stem(term)\n",
    "\n",
    "term_dict = {}\n",
    "\n",
    "for document in data['data_normalized']:\n",
    "    for term in document:\n",
    "        if term not in term_dict:\n",
    "            term_dict[term] = ' '\n",
    "            \n",
    "print(len(term_dict))\n",
    "print(\"------------------------\")\n",
    "\n",
    "for term in term_dict:\n",
    "    if term != stemmed_wrapper(term):\n",
    "        print(term,\":\" ,stemmed_wrapper(term))\n",
    "    term_dict[term] = stemmed_wrapper(term)\n",
    "    \n",
    "    \n",
    "print(term_dict)\n",
    "print(\"------------------------\")\n",
    "\n",
    "\n",
    "# apply stemmed term to dataframe\n",
    "def get_stemmed_term(document):\n",
    "    return [term_dict[term] for term in document]\n",
    "\n",
    "data['data_tokens_stemmed'] = data['data_normalized'].apply(get_stemmed_term)\n",
    "print(data['data_tokens_stemmed'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data[\"join\"] = data.data_tokens_stemmed.apply(lambda x : \" \".join(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------- TF-IDF on Tweet data -------\n",
      "TF-IDF  <class 'numpy.ndarray'> (5, 15)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# banyaknya term yang akan digunakan, \n",
    "# di pilih berdasarkan top max_features \n",
    "# yang diurutkan berdasarkan term frequency seluruh corpus\n",
    "max_features = 1000\n",
    "\n",
    "# Feature Engineering \n",
    "print (\"------- TF-IDF on Tweet data -------\")\n",
    "\n",
    "tf_idf = TfidfVectorizer(max_features=max_features, binary=True)\n",
    "tfidf_mat = tf_idf.fit_transform(data[\"join\"]).toarray()\n",
    "\n",
    "print(\"TF-IDF \", type(tfidf_mat), tfidf_mat.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>term</th>\n",
       "      <th>rank</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>jual</td>\n",
       "      <td>1.236380</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>harga</td>\n",
       "      <td>0.922598</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>murah</td>\n",
       "      <td>0.922598</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>kirim</td>\n",
       "      <td>0.900528</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>mahal</td>\n",
       "      <td>0.617893</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>order</td>\n",
       "      <td>0.614189</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>udah</td>\n",
       "      <td>0.614189</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>cari</td>\n",
       "      <td>0.549036</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>mending</td>\n",
       "      <td>0.549036</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>sebel</td>\n",
       "      <td>0.549036</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>beli</td>\n",
       "      <td>0.525644</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>wajib</td>\n",
       "      <td>0.525644</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>cepat</td>\n",
       "      <td>0.501992</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>keren</td>\n",
       "      <td>0.501992</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>ramah</td>\n",
       "      <td>0.501992</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       term      rank\n",
       "4      jual  1.236380\n",
       "3     harga  0.922598\n",
       "9     murah  0.922598\n",
       "6     kirim  0.900528\n",
       "7     mahal  0.617893\n",
       "10    order  0.614189\n",
       "13     udah  0.614189\n",
       "1      cari  0.549036\n",
       "8   mending  0.549036\n",
       "12    sebel  0.549036\n",
       "0      beli  0.525644\n",
       "14    wajib  0.525644\n",
       "2     cepat  0.501992\n",
       "5     keren  0.501992\n",
       "11    ramah  0.501992"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "terms = tf_idf.get_feature_names()\n",
    "\n",
    "# sum tfidf frequency of each term through documents\n",
    "sums = tfidf_mat.sum(axis=0)\n",
    "\n",
    "# connecting term to its sums frequency\n",
    "data = []\n",
    "for col, term in enumerate(terms):\n",
    "    data.append((term, sums[col] ))\n",
    "\n",
    "ranking = pd.DataFrame(data, columns=['term','rank'])\n",
    "ranking.sort_values('rank', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b3ba2566441a7c06988d0923437866b63cedc61552a5af99d1f4fb67d367b25f"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 64-bit ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
